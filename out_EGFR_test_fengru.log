{'task': 'EGFR_Five_subtyping_fengru', 'split': 'test', 'save_dir': './eval_results/EVAL_EGFR_Five_subtyping_fengru', 'models_dir': 'results/EGFR_Five_subtyping_fengru_s1_old9', 'model_type': 'clam_mb', 'drop_out': 0.25, 'model_size': 'small'}
label column: label
label dictionary: {'Normal': 0, '19DEL': 1, 'L858R': 2, 'Other': 3, 'Wild': 4}
number of classes: 5
slide-level counts:  
 label
0    163
2    183
3    138
1    116
4    142
Name: count, dtype: int64
Patient-LVL; Number of samples registered in class 0: 163
Slide-LVL; Number of samples registered in class 0: 163
Patient-LVL; Number of samples registered in class 1: 116
Slide-LVL; Number of samples registered in class 1: 116
Patient-LVL; Number of samples registered in class 2: 183
Slide-LVL; Number of samples registered in class 2: 183
Patient-LVL; Number of samples registered in class 3: 138
Slide-LVL; Number of samples registered in class 3: 138
Patient-LVL; Number of samples registered in class 4: 142
Slide-LVL; Number of samples registered in class 4: 142
Init Model
CLAM_MB(
  (attention_net): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=5, bias=True)
    )
  )
  (classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=1, bias=True)
  )
  (instance_classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=2, bias=True)
  )
  (instance_loss_fn): CrossEntropyLoss()
)
Total number of parameters: 534292
Total number of trainable parameters: 534292
Init Loaders
test_error:  0.6351351351351351
auc:  0.666539320585483
f1_score:  0.31842250186442145
recall:  0.33432539682539686
confusion_matrix:  [[11.  0.  4.  1.  0.]
 [ 0.  0.  9.  2.  1.]
 [ 0.  0. 10.  6.  2.]
 [ 1.  0.  5.  2.  6.]
 [ 1.  0.  3.  6.  4.]]
Init Model
CLAM_MB(
  (attention_net): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=5, bias=True)
    )
  )
  (classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=1, bias=True)
  )
  (instance_classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=2, bias=True)
  )
  (instance_loss_fn): CrossEntropyLoss()
)
Total number of parameters: 534292
Total number of trainable parameters: 534292
Init Loaders
test_error:  0.5675675675675675
auc:  0.7233913563571517
f1_score:  0.37688888888888894
recall:  0.40674603174603174
confusion_matrix:  [[14.  0.  2.  0.  0.]
 [ 1.  0.  7.  1.  3.]
 [ 2.  3.  8.  2.  3.]
 [ 2.  1.  5.  3.  3.]
 [ 1.  1.  0.  5.  7.]]
Init Model
CLAM_MB(
  (attention_net): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=5, bias=True)
    )
  )
  (classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=1, bias=True)
  )
  (instance_classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=2, bias=True)
  )
  (instance_loss_fn): CrossEntropyLoss()
)
Total number of parameters: 534292
Total number of trainable parameters: 534292
Init Loaders
test_error:  0.5540540540540541
auc:  0.7253329537228312
f1_score:  0.4179540765747662
recall:  0.44563492063492055
confusion_matrix:  [[14.  0.  2.  0.  0.]
 [ 0.  5.  2.  4.  1.]
 [ 2.  6.  4.  2.  4.]
 [ 2.  1.  3.  2.  6.]
 [ 2.  2.  0.  2.  8.]]
Init Model
CLAM_MB(
  (attention_net): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=5, bias=True)
    )
  )
  (classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=1, bias=True)
  )
  (instance_classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=2, bias=True)
  )
  (instance_loss_fn): CrossEntropyLoss()
)
Total number of parameters: 534292
Total number of trainable parameters: 534292
Init Loaders
test_error:  0.5135135135135135
auc:  0.7744436057700796
f1_score:  0.4213557993730408
recall:  0.44980158730158737
confusion_matrix:  [[13.  0.  3.  0.  0.]
 [ 0.  0. 10.  0.  2.]
 [ 0.  0. 13.  2.  3.]
 [ 0.  0.  5.  3.  6.]
 [ 0.  0.  6.  1.  7.]]
Init Model
CLAM_MB(
  (attention_net): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=5, bias=True)
    )
  )
  (classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=1, bias=True)
  )
  (instance_classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=2, bias=True)
  )
  (instance_loss_fn): CrossEntropyLoss()
)
Total number of parameters: 534292
Total number of trainable parameters: 534292
Init Loaders
test_error:  0.5945945945945946
auc:  0.7507048175221145
f1_score:  0.3163059163059163
recall:  0.39841269841269844
confusion_matrix:  [[16.  0.  0.  0.  0.]
 [ 3.  2.  3.  0.  4.]
 [ 4.  3.  2.  1.  8.]
 [ 3.  1.  2.  0.  8.]
 [ 0.  0.  3.  1. 10.]]
Init Model
CLAM_MB(
  (attention_net): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=5, bias=True)
    )
  )
  (classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=1, bias=True)
  )
  (instance_classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=2, bias=True)
  )
  (instance_loss_fn): CrossEntropyLoss()
)
Total number of parameters: 534292
Total number of trainable parameters: 534292
Init Loaders
test_error:  0.6621621621621622
auc:  0.6842867515934812
f1_score:  0.2948809523809524
recall:  0.3240079365079365
confusion_matrix:  [[11.  1.  3.  0.  1.]
 [ 0.  1.  3.  7.  1.]
 [ 1.  2.  5. 10.  0.]
 [ 2.  2.  2.  7.  1.]
 [ 5.  0.  1.  7.  1.]]
Init Model
CLAM_MB(
  (attention_net): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=5, bias=True)
    )
  )
  (classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=1, bias=True)
  )
  (instance_classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=2, bias=True)
  )
  (instance_loss_fn): CrossEntropyLoss()
)
Total number of parameters: 534292
Total number of trainable parameters: 534292
Init Loaders
test_error:  0.6351351351351351
auc:  0.7013862404435264
f1_score:  0.2976929495202336
recall:  0.3371031746031746
confusion_matrix:  [[13.  0.  2.  1.  0.]
 [ 0.  0.  5.  7.  0.]
 [ 2.  0.  8.  8.  0.]
 [ 3.  0.  6.  5.  0.]
 [ 0.  0.  8.  5.  1.]]
Init Model
CLAM_MB(
  (attention_net): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=5, bias=True)
    )
  )
  (classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=1, bias=True)
  )
  (instance_classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=2, bias=True)
  )
  (instance_loss_fn): CrossEntropyLoss()
)
Total number of parameters: 534292
Total number of trainable parameters: 534292
Init Loaders
test_error:  0.581081081081081
auc:  0.694063929056977
f1_score:  0.35044858523119393
recall:  0.39642857142857146
confusion_matrix:  [[10.  0.  3.  3.  0.]
 [ 1.  0.  5.  6.  0.]
 [ 3.  0.  9.  5.  1.]
 [ 0.  0.  3. 10.  1.]
 [ 0.  0.  4.  8.  2.]]
Init Model
CLAM_MB(
  (attention_net): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=5, bias=True)
    )
  )
  (classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=1, bias=True)
  )
  (instance_classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=2, bias=True)
  )
  (instance_loss_fn): CrossEntropyLoss()
)
Total number of parameters: 534292
Total number of trainable parameters: 534292
Init Loaders
test_error:  0.5675675675675675
auc:  0.7337046453731659
f1_score:  0.4132728606316669
recall:  0.4119047619047619
confusion_matrix:  [[12.  1.  3.  0.  0.]
 [ 1.  2.  6.  3.  0.]
 [ 3.  3.  9.  1.  2.]
 [ 0.  1.  9.  3.  1.]
 [ 2.  0.  4.  2.  6.]]
Init Model
CLAM_MB(
  (attention_net): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=5, bias=True)
    )
  )
  (classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=1, bias=True)
  )
  (instance_classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=2, bias=True)
  )
  (instance_loss_fn): CrossEntropyLoss()
)
Total number of parameters: 534292
Total number of trainable parameters: 534292
Init Loaders
test_error:  0.5405405405405406
auc:  0.7838349047442484
f1_score:  0.3805965853571872
recall:  0.4436507936507937
confusion_matrix:  [[12.  1.  1.  0.  2.]
 [ 0.  2.  6.  0.  4.]
 [ 3.  3.  8.  0.  4.]
 [ 3.  2.  2.  0.  7.]
 [ 0.  2.  0.  0. 12.]]
第 0 个 fold_label_b 的形状: (74, 5)
第 1 个 fold_label_b 的形状: (74, 5)
第 2 个 fold_label_b 的形状: (74, 5)
第 3 个 fold_label_b 的形状: (74, 5)
第 4 个 fold_label_b 的形状: (74, 5)
第 5 个 fold_label_b 的形状: (74, 5)
第 6 个 fold_label_b 的形状: (74, 5)
第 7 个 fold_label_b 的形状: (74, 5)
第 8 个 fold_label_b 的形状: (74, 5)
第 9 个 fold_label_b 的形状: (74, 5)
[0.666539320585483, 0.7233913563571517, 0.7253329537228312, 0.7744436057700796, 0.7507048175221145, 0.6842867515934812, 0.7013862404435264, 0.694063929056977, 0.7337046453731659, 0.7838349047442484]
