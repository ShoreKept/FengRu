{'task': 'EGFR_Five_subtyping_fengru', 'split': 'test', 'save_dir': './eval_results/EVAL_EGFR_Five_UNI_d1024', 'models_dir': 'results/EGFR_Five_UNI_d1024_old3/EGFR_Five_subtyping_fengru_s1', 'model_type': 'clam_mb', 'drop_out': 0.25, 'model_size': 'small'}
label column: label
label dictionary: {'Normal': 0, '19DEL': 1, 'L858R': 2, 'Other': 3, 'Wild': 4}
number of classes: 5
slide-level counts:  
 label
0    163
2    183
3    138
1    116
4    142
Name: count, dtype: int64
Patient-LVL; Number of samples registered in class 0: 163
Slide-LVL; Number of samples registered in class 0: 163
Patient-LVL; Number of samples registered in class 1: 116
Slide-LVL; Number of samples registered in class 1: 116
Patient-LVL; Number of samples registered in class 2: 183
Slide-LVL; Number of samples registered in class 2: 183
Patient-LVL; Number of samples registered in class 3: 138
Slide-LVL; Number of samples registered in class 3: 138
Patient-LVL; Number of samples registered in class 4: 142
Slide-LVL; Number of samples registered in class 4: 142
Init Model
CLAM_MB(
  (attention_net): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=5, bias=True)
    )
  )
  (classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=1, bias=True)
  )
  (instance_classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=2, bias=True)
  )
  (instance_loss_fn): CrossEntropyLoss()
)
Total number of parameters: 796436
Total number of trainable parameters: 796436
Init Loaders
test_error:  0.6216216216216216
auc:  0.7312847829157617
f1_score:  0.37791977436026375
recall:  0.3684523809523809
confusion_matrix:  [[11.  0.  4.  1.  0.]
 [ 0.  3.  8.  0.  1.]
 [ 0.  4.  6.  4.  4.]
 [ 1.  1.  0.  4.  8.]
 [ 1.  2.  1.  6.  4.]]
Init Model
CLAM_MB(
  (attention_net): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=5, bias=True)
    )
  )
  (classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=1, bias=True)
  )
  (instance_classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=2, bias=True)
  )
  (instance_loss_fn): CrossEntropyLoss()
)
Total number of parameters: 796436
Total number of trainable parameters: 796436
Init Loaders
test_error:  0.4864864864864865
auc:  0.8213414596465208
f1_score:  0.43077989090373
recall:  0.49146825396825394
confusion_matrix:  [[15.  0.  1.  0.  0.]
 [ 2.  3.  6.  0.  1.]
 [ 2.  2. 10.  0.  4.]
 [ 2.  2.  5.  0.  5.]
 [ 1.  1.  2.  0. 10.]]
Init Model
CLAM_MB(
  (attention_net): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=5, bias=True)
    )
  )
  (classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=1, bias=True)
  )
  (instance_classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=2, bias=True)
  )
  (instance_loss_fn): CrossEntropyLoss()
)
Total number of parameters: 796436
Total number of trainable parameters: 796436
Init Loaders
test_error:  0.5405405405405406
auc:  0.8104150555290712
f1_score:  0.43703923278116824
recall:  0.47281746031746036
confusion_matrix:  [[13.  1.  2.  0.  0.]
 [ 0.  7.  1.  4.  0.]
 [ 0. 11.  2.  3.  2.]
 [ 1.  2.  1.  9.  1.]
 [ 1.  2.  1.  7.  3.]]
Init Model
CLAM_MB(
  (attention_net): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=5, bias=True)
    )
  )
  (classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=1, bias=True)
  )
  (instance_classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=2, bias=True)
  )
  (instance_loss_fn): CrossEntropyLoss()
)
Total number of parameters: 796436
Total number of trainable parameters: 796436
Init Loaders
test_error:  0.527027027027027
auc:  0.843109517629818
f1_score:  0.4617514475085689
recall:  0.4640873015873016
confusion_matrix:  [[13.  0.  3.  0.  0.]
 [ 1.  4.  6.  1.  0.]
 [ 2.  6.  7.  1.  2.]
 [ 0.  1.  1.  6.  6.]
 [ 1.  0.  1.  7.  5.]]
Init Model
CLAM_MB(
  (attention_net): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=5, bias=True)
    )
  )
  (classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=1, bias=True)
  )
  (instance_classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=2, bias=True)
  )
  (instance_loss_fn): CrossEntropyLoss()
)
Total number of parameters: 796436
Total number of trainable parameters: 796436
Init Loaders
test_error:  0.5135135135135135
auc:  0.7510186803679574
f1_score:  0.4475820188581099
recall:  0.47579365079365077
confusion_matrix:  [[14.  0.  2.  0.  0.]
 [ 2.  1.  3.  4.  2.]
 [ 0.  1.  5.  9.  3.]
 [ 2.  0.  2.  7.  3.]
 [ 0.  0.  2.  3.  9.]]
Init Model
CLAM_MB(
  (attention_net): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=5, bias=True)
    )
  )
  (classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=1, bias=True)
  )
  (instance_classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=2, bias=True)
  )
  (instance_loss_fn): CrossEntropyLoss()
)
Total number of parameters: 796436
Total number of trainable parameters: 796436
Init Loaders
test_error:  0.47297297297297297
auc:  0.7984321644861134
f1_score:  0.5321257763271227
recall:  0.5162698412698412
confusion_matrix:  [[10.  1.  5.  0.  0.]
 [ 0.  5.  4.  3.  0.]
 [ 0.  1. 11.  3.  3.]
 [ 0.  0.  3.  7.  4.]
 [ 1.  0.  2.  5.  6.]]
Init Model
CLAM_MB(
  (attention_net): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=5, bias=True)
    )
  )
  (classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=1, bias=True)
  )
  (instance_classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=2, bias=True)
  )
  (instance_loss_fn): CrossEntropyLoss()
)
Total number of parameters: 796436
Total number of trainable parameters: 796436
Init Loaders
test_error:  0.6081081081081081
auc:  0.7259690882285431
f1_score:  0.29562222423638174
recall:  0.3498015873015873
confusion_matrix:  [[13.  0.  3.  0.  0.]
 [ 1.  0.  7.  1.  3.]
 [ 2.  0. 13.  3.  0.]
 [ 2.  0. 10.  0.  2.]
 [ 0.  0.  8.  3.  3.]]
Init Model
CLAM_MB(
  (attention_net): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=5, bias=True)
    )
  )
  (classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=1, bias=True)
  )
  (instance_classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=2, bias=True)
  )
  (instance_loss_fn): CrossEntropyLoss()
)
Total number of parameters: 796436
Total number of trainable parameters: 796436
Init Loaders
test_error:  0.5405405405405406
auc:  0.7919977444073663
f1_score:  0.4564442093853859
recall:  0.4833333333333334
confusion_matrix:  [[8. 1. 3. 2. 2.]
 [1. 9. 2. 0. 0.]
 [1. 8. 3. 2. 4.]
 [0. 2. 0. 6. 6.]
 [0. 1. 1. 4. 8.]]
Init Model
CLAM_MB(
  (attention_net): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=5, bias=True)
    )
  )
  (classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=1, bias=True)
  )
  (instance_classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=2, bias=True)
  )
  (instance_loss_fn): CrossEntropyLoss()
)
Total number of parameters: 796436
Total number of trainable parameters: 796436
Init Loaders
test_error:  0.581081081081081
auc:  0.725165682327807
f1_score:  0.39705387205387205
recall:  0.40515873015873016
confusion_matrix:  [[10.  2.  4.  0.  0.]
 [ 1.  5.  6.  0.  0.]
 [ 4.  2. 10.  1.  1.]
 [ 0.  2.  5.  3.  4.]
 [ 1.  4.  2.  4.  3.]]
Init Model
CLAM_MB(
  (attention_net): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=5, bias=True)
    )
  )
  (classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=1, bias=True)
  )
  (instance_classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=2, bias=True)
  )
  (instance_loss_fn): CrossEntropyLoss()
)
Total number of parameters: 796436
Total number of trainable parameters: 796436
Init Loaders
test_error:  0.5405405405405406
auc:  0.8059622243409785
f1_score:  0.4432362232362232
recall:  0.46269841269841266
confusion_matrix:  [[12.  1.  2.  0.  1.]
 [ 0.  6.  6.  0.  0.]
 [ 5.  3.  5.  3.  2.]
 [ 4.  2.  2.  2.  4.]
 [ 0.  0.  2.  3.  9.]]
第 0 个 fold_label_b 的形状: (74, 5)
第 1 个 fold_label_b 的形状: (74, 5)
第 2 个 fold_label_b 的形状: (74, 5)
第 3 个 fold_label_b 的形状: (74, 5)
第 4 个 fold_label_b 的形状: (74, 5)
第 5 个 fold_label_b 的形状: (74, 5)
第 6 个 fold_label_b 的形状: (74, 5)
第 7 个 fold_label_b 的形状: (74, 5)
第 8 个 fold_label_b 的形状: (74, 5)
第 9 个 fold_label_b 的形状: (74, 5)
[0.7312847829157617, 0.8213414596465208, 0.8104150555290712, 0.843109517629818, 0.7510186803679574, 0.7984321644861134, 0.7259690882285431, 0.7919977444073663, 0.725165682327807, 0.8059622243409785]
