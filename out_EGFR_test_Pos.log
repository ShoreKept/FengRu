{'task': 'EGFR_Five_subtyping_fengru', 'split': 'test', 'save_dir': './eval_results/EVAL_EGFR_Five_UNI_d1024_Pos', 'models_dir': 'results/N_EGFR_Five_UNI_d1024/N_EGFR_Five_subtyping_fengru_s1', 'model_type': 'clam_mb', 'drop_out': 0.25, 'model_size': 'small'}
label column: label
label dictionary: {'Normal': 0, '19DEL': 1, 'L858R': 2, 'Other': 3, 'Wild': 4}
number of classes: 5
slide-level counts:  
 label
0    163
2    183
3    138
1    116
4    142
Name: count, dtype: int64
Patient-LVL; Number of samples registered in class 0: 163
Slide-LVL; Number of samples registered in class 0: 163
Patient-LVL; Number of samples registered in class 1: 116
Slide-LVL; Number of samples registered in class 1: 116
Patient-LVL; Number of samples registered in class 2: 183
Slide-LVL; Number of samples registered in class 2: 183
Patient-LVL; Number of samples registered in class 3: 138
Slide-LVL; Number of samples registered in class 3: 138
Patient-LVL; Number of samples registered in class 4: 142
Slide-LVL; Number of samples registered in class 4: 142
Init Model
CLAM_MB(
  (attention_net): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=5, bias=True)
    )
  )
  (classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=1, bias=True)
  )
  (instance_classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=2, bias=True)
  )
  (instance_loss_fn): CrossEntropyLoss()
)
Total number of parameters: 796436
Total number of trainable parameters: 796436
Init Loaders
test_error:  0.5540540540540541
auc:  0.7449465455444321
f1_score:  0.45308379932960535
recall:  0.43730158730158736
confusion_matrix:  [[ 8.  0.  5.  3.  0.]
 [ 0.  5.  6.  1.  0.]
 [ 0.  1. 10.  1.  6.]
 [ 0.  1.  3.  3.  7.]
 [ 1.  0.  1.  5.  7.]]
Init Model
CLAM_MB(
  (attention_net): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=5, bias=True)
    )
  )
  (classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=1, bias=True)
  )
  (instance_classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=2, bias=True)
  )
  (instance_loss_fn): CrossEntropyLoss()
)
Total number of parameters: 796436
Total number of trainable parameters: 796436
Init Loaders
test_error:  0.4864864864864865
auc:  0.8098528347193531
f1_score:  0.44730930421083226
recall:  0.4851190476190476
confusion_matrix:  [[15.  0.  1.  0.  0.]
 [ 2.  3.  6.  0.  1.]
 [ 1.  0. 12.  1.  4.]
 [ 2.  2.  4.  1.  5.]
 [ 1.  2.  4.  0.  7.]]
Init Model
CLAM_MB(
  (attention_net): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=5, bias=True)
    )
  )
  (classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=1, bias=True)
  )
  (instance_classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=2, bias=True)
  )
  (instance_loss_fn): CrossEntropyLoss()
)
Total number of parameters: 796436
Total number of trainable parameters: 796436
Init Loaders
test_error:  0.5540540540540541
auc:  0.7744740849621272
f1_score:  0.43619393310863364
recall:  0.43571428571428567
confusion_matrix:  [[12.  0.  4.  0.  0.]
 [ 0.  6.  6.  0.  0.]
 [ 0.  7.  9.  0.  2.]
 [ 0.  8.  4.  2.  0.]
 [ 1.  2.  4.  3.  4.]]
Init Model
CLAM_MB(
  (attention_net): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=5, bias=True)
    )
  )
  (classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=1, bias=True)
  )
  (instance_classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=2, bias=True)
  )
  (instance_loss_fn): CrossEntropyLoss()
)
Total number of parameters: 796436
Total number of trainable parameters: 796436
Init Loaders
test_error:  0.5540540540540541
auc:  0.8404651111464236
f1_score:  0.44666666666666666
recall:  0.4636904761904762
confusion_matrix:  [[11.  4.  1.  0.  0.]
 [ 0.  9.  3.  0.  0.]
 [ 1. 11.  3.  2.  1.]
 [ 0.  6.  0.  6.  2.]
 [ 0.  3.  2.  5.  4.]]
Init Model
CLAM_MB(
  (attention_net): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=5, bias=True)
    )
  )
  (classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=1, bias=True)
  )
  (instance_classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=2, bias=True)
  )
  (instance_loss_fn): CrossEntropyLoss()
)
Total number of parameters: 796436
Total number of trainable parameters: 796436
Init Loaders
test_error:  0.581081081081081
auc:  0.7995969507565726
f1_score:  0.4238277686553548
recall:  0.42103174603174603
confusion_matrix:  [[10.  1.  5.  0.  0.]
 [ 2.  5.  3.  1.  1.]
 [ 0.  6.  5.  4.  3.]
 [ 1.  2.  2.  5.  4.]
 [ 0.  1.  2.  5.  6.]]
Init Model
CLAM_MB(
  (attention_net): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=5, bias=True)
    )
  )
  (classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=1, bias=True)
  )
  (instance_classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=2, bias=True)
  )
  (instance_loss_fn): CrossEntropyLoss()
)
Total number of parameters: 796436
Total number of trainable parameters: 796436
Init Loaders
test_error:  0.6081081081081081
auc:  0.7670959134488056
f1_score:  0.3622424242424242
recall:  0.41111111111111115
confusion_matrix:  [[ 8.  2.  6.  0.  0.]
 [ 0. 10.  2.  0.  0.]
 [ 0.  9.  4.  4.  1.]
 [ 0.  4.  1.  7.  2.]
 [ 1.  7.  1.  5.  0.]]
Init Model
CLAM_MB(
  (attention_net): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=5, bias=True)
    )
  )
  (classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=1, bias=True)
  )
  (instance_classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=2, bias=True)
  )
  (instance_loss_fn): CrossEntropyLoss()
)
Total number of parameters: 796436
Total number of trainable parameters: 796436
Init Loaders
test_error:  0.5675675675675675
auc:  0.717313593587231
f1_score:  0.4068979591836735
recall:  0.4095238095238095
confusion_matrix:  [[12.  0.  3.  0.  1.]
 [ 0.  1.  6.  3.  2.]
 [ 2.  1.  9.  6.  0.]
 [ 2.  0.  8.  3.  1.]
 [ 0.  0.  5.  2.  7.]]
Init Model
CLAM_MB(
  (attention_net): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=5, bias=True)
    )
  )
  (classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=1, bias=True)
  )
  (instance_classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=2, bias=True)
  )
  (instance_loss_fn): CrossEntropyLoss()
)
Total number of parameters: 796436
Total number of trainable parameters: 796436
Init Loaders
test_error:  0.5540540540540541
auc:  0.7603939121069265
f1_score:  0.42305882352941176
recall:  0.42202380952380947
confusion_matrix:  [[ 9.  0.  5.  1.  1.]
 [ 1.  2.  9.  0.  0.]
 [ 1.  1. 12.  3.  1.]
 [ 0.  2.  2.  6.  4.]
 [ 0.  0.  4.  6.  4.]]
Init Model
CLAM_MB(
  (attention_net): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=5, bias=True)
    )
  )
  (classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=1, bias=True)
  )
  (instance_classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=2, bias=True)
  )
  (instance_loss_fn): CrossEntropyLoss()
)
Total number of parameters: 796436
Total number of trainable parameters: 796436
Init Loaders
test_error:  0.5540540540540541
auc:  0.7300930707841163
f1_score:  0.41334253508166546
recall:  0.4138888888888889
confusion_matrix:  [[10.  2.  4.  0.  0.]
 [ 1.  2.  8.  1.  0.]
 [ 0.  2. 14.  1.  1.]
 [ 0.  1.  6.  4.  3.]
 [ 0.  4.  4.  3.  3.]]
Init Model
CLAM_MB(
  (attention_net): Sequential(
    (0): Linear(in_features=1024, out_features=512, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.25, inplace=False)
    (3): Attn_Net_Gated(
      (attention_a): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Tanh()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_b): Sequential(
        (0): Linear(in_features=512, out_features=256, bias=True)
        (1): Sigmoid()
        (2): Dropout(p=0.25, inplace=False)
      )
      (attention_c): Linear(in_features=256, out_features=5, bias=True)
    )
  )
  (classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=1, bias=True)
  )
  (instance_classifiers): ModuleList(
    (0-4): 5 x Linear(in_features=512, out_features=2, bias=True)
  )
  (instance_loss_fn): CrossEntropyLoss()
)
Total number of parameters: 796436
Total number of trainable parameters: 796436
Init Loaders
test_error:  0.4864864864864865
auc:  0.8314849391740383
f1_score:  0.49194942988046436
recall:  0.5007936507936508
confusion_matrix:  [[12.  1.  2.  0.  1.]
 [ 0.  6.  6.  0.  0.]
 [ 1.  4. 11.  2.  0.]
 [ 4.  1.  3.  6.  0.]
 [ 0.  2.  2.  7.  3.]]
第 0 个 fold_label_b 的形状: (74, 5)
第 1 个 fold_label_b 的形状: (74, 5)
第 2 个 fold_label_b 的形状: (74, 5)
第 3 个 fold_label_b 的形状: (74, 5)
第 4 个 fold_label_b 的形状: (74, 5)
第 5 个 fold_label_b 的形状: (74, 5)
第 6 个 fold_label_b 的形状: (74, 5)
第 7 个 fold_label_b 的形状: (74, 5)
第 8 个 fold_label_b 的形状: (74, 5)
第 9 个 fold_label_b 的形状: (74, 5)
[0.7449465455444321, 0.8098528347193531, 0.7744740849621272, 0.8404651111464236, 0.7995969507565726, 0.7670959134488056, 0.717313593587231, 0.7603939121069265, 0.7300930707841163, 0.8314849391740383]
